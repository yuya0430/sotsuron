モデルの実装には Tensorflow を用いている．実行環境は，産業技術総合研究所が構築し運用する，人工知能処理向け計算インフラストラクチャであるAI橋渡しクラウド（AI Bridging Cloud Infrastructure; ABCI）\cite{abci}を利用している．ABCIの計算資源はタイプ別に分かれているが，本研究では，CPU（Intel Xeon Gold 6148プロセッサー 2.4 GHz）が 5コア，GPU（NVIDIA Tesla V100 for NVLink 16GiB HBM2）が 1 個，メモリが 240 GiB，ローカルストレージが 180 GBの G.small を用いている．
\par
モデルの設定は，与えられたベースラインモデルとミニバッチサイズ以外全て同じにしている．BERT のハイパーパラメータはBERTの論文\cite{bert}に記載された $\verb|BERT|_{\verb|BASE|}$ の構成をそのまま使用する．つまり，BERTは隠れ層が 12 層，隠れ層のサイズが 768 次元である．学習はミニバッチ学習を行い，ミニバッチサイズを学習時は 32，検証時は 8，テスト時は 8 とする．ただし，従来の対話履歴の使用法との比較の際は，学習時のミニバッチサイズを 24 にして実験を行う．入力発話の最大長は，2発話入力の場合で 80 とし，1 発話増えるたびに 40 増加させる．学習率は 1e-4 とする．誤差関数には，交差エントロピーを使用し，最適化アルゴリズムは重み減衰によって過学習を抑制する Weight Decay という手法を用いた Adam \cite{adam} を使用する．過学習を防ぎモデルの汎化性能を上げるために，dropout（dropout 率は 0.1）を用いる．モデルは 80 epoch の学習を行い，学習を終えた段階のモデルを評価に使用する．
\par
データは Single-domain のみを用いて，学習用として 5,403 対話，検証用として 836 対話を学習に用いる．テスト用のデータはまだ未公開のため，今回は検証用データで評価している．
